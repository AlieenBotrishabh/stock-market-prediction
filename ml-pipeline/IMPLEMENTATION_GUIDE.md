# Stock Price Prediction Pipeline - Complete Implementation Guide

## Overview

This is a **production-ready Python machine learning pipeline** for:
1. **Efficiently collecting** stock market data from IndianAPI (respecting 500 requests/month quota)
2. **Processing data** with technical indicators (SMA, EMA, RSI)
3. **Training LSTM models** to predict next-day stock prices
4. **Preventing quota burnout** with comprehensive logging and rate limiting

**Key Features:**
- ‚úÖ Clean modular architecture (separation of concerns)
- ‚úÖ Rate limiting to prevent quota waste
- ‚úÖ All TODO items marked with exact points to update
- ‚úÖ Comprehensive error handling and logging
- ‚úÖ No extra API calls during training (use saved data only)
- ‚úÖ Time-series data handling (chronological train/test split)

---

## What You're Getting

```
ml-pipeline/
‚îú‚îÄ‚îÄ config.py              # 1. CENTRAL CONFIG - API keys, symbols, parameters
‚îú‚îÄ‚îÄ api_client.py          # 2. DATA COLLECTION - API calls with rate limiting  
‚îú‚îÄ‚îÄ data_processor.py      # 3. FEATURE ENGINEERING - Technical indicators
‚îú‚îÄ‚îÄ model.py               # 4. MODEL TRAINING - LSTM architecture
‚îú‚îÄ‚îÄ main.py                # 5. ORCHESTRATION - CLI to run pipeline
‚îú‚îÄ‚îÄ requirements.txt       # Dependencies
‚îú‚îÄ‚îÄ .env.example           # Environment variable template
‚îú‚îÄ‚îÄ __init__.py            # Python package initialization
‚îî‚îÄ‚îÄ README.md              # User-friendly documentation

Directories created automatically:
‚îú‚îÄ‚îÄ data/raw/              # Raw API responses (JSON)
‚îú‚îÄ‚îÄ data/processed/        # Processed datasets (CSV)
‚îú‚îÄ‚îÄ models/                # Trained model checkpoints
‚îî‚îÄ‚îÄ logs/                  # Execution logs
```

---

## üöÄ Getting Started

### Step 1: Setup Environment

```bash
# Navigate to ml-pipeline directory
cd ml-pipeline

# Create virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # Linux/macOS
# OR
venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt
```

### Step 2: Configure API Key

```bash
# Copy template
cp .env.example .env

# Edit .env with your IndianAPI key
nano .env  # or use your editor

# Set environment variable (alternative)
export INDIANAPI_KEY="your_key_here"
```

### Step 3: Update Configuration (CRITICAL)

Edit `config.py` and fill in TODO items from IndianAPI documentation:

**TODO #1: Verify API Endpoints**
```python
# Line ~35 in config.py
API_ENDPOINTS = {
    "historical": "/api/???",  # REPLACE with actual endpoint
    "quote": "/api/???",
    "company": "/api/???",
}
```

**TODO #2: Verify Symbol Format**
```python
# Line ~20 in config.py
SYMBOLS = [
    "TCS",      # Verify format - might be "TCS.NS", "NSE:TCS", etc.
    "HDFC",
    "RELIANCE",
]
```

**TODO #3: Verify Response Structure**
```python
# In api_client.py, line ~95 - check authentication method
# In data_processor.py, line ~50 - update JSON parsing based on API response
```

### Step 4: Verify Configuration

```bash
# Check if API key is set
python -c "import os; print('API Key:', 'SET' if os.getenv('INDIANAPI_KEY') else 'NOT SET')"

# Validate configuration
python -c "from config import Config; Config.validate()"
```

---

## üìä Using the Pipeline

### Command: Check Rate Limit

```bash
python main.py --status
```

**Output:**
```
Monthly quota (500 requests):
  Used: 25
  Remaining: 475
  Percentage: 5.0%

Today's quota (10 requests):
  Used: 3
  Remaining: 7
```

**What it does:**
- Reads `logs/request_log.json` (no API calls)
- Shows monthly and daily usage
- Safe to run anytime

### Command: Collect Data

```bash
python main.py --collect
```

**What it does:**
1. Checks rate limit (stops if exhausted)
2. For each symbol:
   - Makes 1 API request for historical data
   - Saves raw JSON to `data/raw/{symbol}_raw.json`
   - Logs all request details
3. Displays summary (success/failed symbols)

**Rate limiting:**
- Allows 10 requests/day (conservative for 500/month quota)
- Daily quota resets at midnight UTC
- Each symbol = 1 API request

**Output:**
```
Rate Limit Status:
  Total requests this month: 5/500
  Requests today: 2/10
  Remaining today: 8

Fetching historical data for TCS...
‚úì Successfully collected data for TCS
Fetching historical data for HDFC...
‚úì Successfully collected data for HDFC
...

Data collection complete: 5/5 symbols
```

### Command: Process Data

```bash
python main.py --process
```

**What it does:**
1. For each symbol:
   - Loads raw JSON from `data/raw/`
   - Parses into DataFrame with columns: timestamp, open, high, low, close, volume
   - Computes technical indicators:
     - SMA (5, 10, 20 day)
     - EMA (5, 10, 20 day)
     - RSI (14 day)
     - ROC (rate of change)
     - Volatility
   - Saves processed CSV to `data/processed/{symbol}_processed.csv`

2. Validates data:
   - Removes rows with missing values
   - Ensures minimum data points (30+)
   - Handles outliers

**Output:**
```
Processing TCS...
‚úì Processed 57 valid data points for TCS
‚úì Computed 11 technical indicators

Processing HDFC...
‚úì Processed 59 valid data points for HDFC
‚úì Computed 11 technical indicators
...
```

**Generated features per symbol:**
- OHLCV columns: open, high, low, close, volume
- Moving averages: sma_5, sma_10, sma_20, ema_5, ema_10, ema_20
- Momentum: rsi_14, roc, volatility, return
- Total: ~15-20 features per time step

### Command: Train Models

```bash
python main.py --train
```

**What it does:**
1. For each symbol:
   - Loads processed CSV
   - Creates sequences:
     - Input: last 10 days of features (10 √ó 13 features)
     - Output: next day's close price
   - Normalizes features (zero mean, unit variance)
   - Splits: 70% train, 20% validation, 10% test (chronological order)
   - Trains LSTM model:
     - 2 LSTM layers (64 units each)
     - Dropout 0.2 (prevent overfitting)
     - Adam optimizer
     - Early stopping (stop if val_loss doesn't improve)
   - Evaluates on test set:
     - MAE, RMSE, MAPE metrics
   - Saves trained model to `models/{symbol}.h5`

**Output:**
```
Training model for TCS
================================================

Creating 47 sequences (length=10)
‚úì Created 47 sequences
  X shape: (47, 10, 13), y shape: (47,)

‚úì Dataset split:
  Train: 33 samples (70.2%)
  Validation: 9 samples (19.1%)
  Test: 5 samples (10.6%)

Epoch 1/50: loss=0.0234, val_loss=0.0198
Epoch 2/50: loss=0.0189, val_loss=0.0156
...
Epoch 45/50: loss=0.0045, val_loss=0.0052
Early stopping triggered at epoch 45

‚úì Training complete
  Final loss: 0.0045
  Final val_loss: 0.0052
  Final MAE: 0.0032
  Final val_MAE: 0.0038

‚úì Test Results:
  Loss (MSE): 0.0062
  MAE: 0.0048
  RMSE: 0.0787
  MAPE: 0.52%

‚úì Saved model to models/TCS.h5
‚úì Saved metadata to models/TCS.json
```

**Time complexity:**
- Data processing: ~5 seconds per symbol
- Model training: ~30-60 seconds per symbol
- Total for 5 symbols: ~5-10 minutes

### Command: Full Pipeline

```bash
python main.py --full
```

**What it does:**
- Runs all steps in sequence:
  1. Collect data (1 API request per symbol)
  2. Process data (compute features)
  3. Train models (LSTM training)

**Total time:** ~15-20 minutes for 5 symbols

---

## üìÇ File Structure Explained

### `config.py` - Central Configuration

**Purpose:** Single source of truth for all parameters

**Key sections:**
```python
# API Configuration
INDIANAPI_KEY = os.getenv("INDIANAPI_KEY")        # Your API key
INDIANAPI_BASE_URL = "https://api.indianapi.com"  # Base URL
MAX_REQUESTS_PER_DAY = 10                         # Rate limiting

# Symbols and Data
SYMBOLS = ["TCS", "HDFC", "RELIANCE", "WIPRO", "INFY"]
HISTORY_DAYS = 60                                 # Days of history to fetch

# Model Parameters
LSTM_UNITS = 64                                   # LSTM cell size
EPOCHS = 50                                       # Training epochs
SEQUENCE_LENGTH = 10                              # Look back 10 days
BATCH_SIZE = 32

# Feature Engineering
MA_PERIODS = [5, 10, 20]                         # Moving average periods
TECHNICAL_INDICATORS = ["SMA", "EMA", "RSI"]
```

**TODO items marked in code:**
- Line 35: API endpoints
- Line 20: Symbol format verification
- Line 50: Response structure

### `api_client.py` - Data Collection with Rate Limiting

**Key Classes:**

1. **RateLimiter** - Prevents quota burnout
   - Tracks requests in `logs/request_log.json`
   - Enforces daily limits (10/day)
   - Monthly quota tracking (500/month)
   - Automatic reset at midnight UTC

2. **IndianAPIClient** - API communication
   - Makes authenticated requests
   - Retry logic with exponential backoff
   - Saves raw JSON locally
   - Comprehensive error handling

**Key Methods:**
```python
client = IndianAPIClient()

# Fetch historical data (1 request)
data = client.get_historical_data("TCS", days=60)

# Get real-time quote (1 request)
quote = client.get_quote("TCS")

# Check rate limit status (0 requests)
stats = client.get_rate_limit_stats()

# Load previously saved data (0 requests)
data = client.load_raw_data("TCS")
```

**Error Handling:**
- Network timeouts: Retry with exponential backoff
- Server rate limits (429): Back off 5-30 seconds
- Invalid response: Log and skip symbol
- All errors logged to `logs/api_client.log`

### `data_processor.py` - Feature Engineering

**Key Class: DataProcessor**

**Methods:**
```python
processor = DataProcessor()

# Parse raw JSON ‚Üí DataFrame
df = processor.process_raw_data(raw_json, symbol="TCS")
# Returns: DataFrame with columns [timestamp, open, high, low, close, volume]

# Add technical indicators
df = processor.compute_technical_indicators(df)
# Returns: Same DataFrame + new columns [sma_5, ema_10, rsi_14, ...]

# Create sequences for LSTM
X, y = processor.create_sequences(df, sequence_length=10, target_col="close")
# Returns: X shape (47, 10, 13), y shape (47,)
# X[0] = last 10 days of all features ‚Üí y[0] = next day's close price

# Normalize features
X = processor.normalize_data(X, fit=True)
# Returns: Normalized X (zero mean, unit variance)

# Split dataset chronologically
X_train, X_val, X_test, y_train, y_val, y_test = processor.split_dataset(X, y)
# Prevents data leakage: [Past data] | [Recent data] | [Future data]
```

**Technical Indicators Computed:**
```
Input: OHLCV (Open, High, Low, Close, Volume)
‚Üì
Simple Moving Averages: SMA-5, SMA-10, SMA-20
Exponential Moving Averages: EMA-5, EMA-10, EMA-20
Relative Strength Index: RSI-14
Rate of Change: ROC
Volatility: Rolling std of returns
Daily Return: Percentage change
‚Üì
Output: ~15-20 features per time step
```

### `model.py` - LSTM Model

**Key Class: StockPricePredictor**

**Architecture:**
```
Input (batch, 10, 13)
    ‚Üì
LSTM layer 1 (64 units) ‚Üí return_sequences=True
    ‚Üì
Dropout (0.2)
    ‚Üì
LSTM layer 2 (64 units)
    ‚Üì
Dropout (0.2)
    ‚Üì
Dense layer (32 units, ReLU activation)
    ‚Üì
Dropout (0.2)
    ‚Üì
Output (1 unit) ‚Üí Price prediction
```

**Key Methods:**
```python
# Create model
model = StockPricePredictor(input_shape=(10, 13))

# Train
results = model.train(X_train, y_train, X_val, y_val, symbol="TCS")
# Returns: {"loss": 0.045, "val_loss": 0.052, "mae": 0.032, ...}

# Evaluate
metrics = model.evaluate(X_test, y_test)
# Returns: {"loss": 0.062, "mae": 0.048, "rmse": 0.079, "mape": 0.52, ...}

# Predict
predictions = model.predict(X_new)
# Returns: array of predicted prices

# Save/Load
model.save("TCS")
loaded_model = StockPricePredictor.load("TCS")
```

**Evaluation Metrics:**
- **MAE** (Mean Absolute Error): Average absolute difference (in price units)
- **RMSE** (Root Mean Squared Error): Punishes large errors more heavily
- **MAPE** (Mean Absolute Percentage Error): Percentage error (more interpretable)
- **Loss (MSE)**: Mean Squared Error used during training

### `main.py` - Orchestration

**Entry point with CLI:**
```bash
python main.py --collect     # Collect data
python main.py --process     # Process data
python main.py --train       # Train models
python main.py --full        # All three
python main.py --status      # Check quota
python main.py --help        # Show help
```

**Functions:**
- `collect_data()` - Orchestrates data collection
- `process_data()` - Orchestrates data processing
- `train_models()` - Orchestrates model training
- `show_status()` - Display rate limit stats
- `main()` - CLI entry point

---

## üîç Code Walkthrough Examples

### Example 1: Collecting Data for One Symbol

```python
from api_client import IndianAPIClient

client = IndianAPIClient()

# Fetch historical data for TCS
# This makes 1 HTTP request, saves raw JSON locally
data = client.get_historical_data("TCS", days=60)

# Check how many requests remain today
stats = client.get_rate_limit_stats()
print(f"Remaining requests today: {stats['remaining_today']}/10")
```

**What happens:**
1. Check if daily limit (10) not reached
2. Make GET request to `/api/historical?symbol=TCS&period=60d`
3. Save response to `data/raw/TCS_raw.json`
4. Log request to `logs/request_log.json`
5. Return parsed JSON

### Example 2: Processing Data to Features

```python
from api_client import IndianAPIClient
from data_processor import DataProcessor

client = IndianAPIClient()
processor = DataProcessor()

# Load raw data
raw_data = client.load_raw_data("TCS")

# Convert to DataFrame with OHLCV
df = processor.process_raw_data(raw_data, "TCS")
print(df.columns)  # [timestamp, open, high, low, close, volume]
print(len(df))     # e.g., 57 rows (57 days of data)

# Add technical indicators
df = processor.compute_technical_indicators(df)
print(df.columns)  # [timestamp, open, high, ..., sma_5, ema_10, rsi_14, ...]

# Create sequences
X, y = processor.create_sequences(df, sequence_length=10)
print(X.shape)  # (47, 10, 13) - 47 sequences of 10 days √ó 13 features
print(y.shape)  # (47,) - 47 target prices
```

**What happens:**
1. Parse JSON to DataFrame
2. Compute 11 technical indicators
3. Create sliding window:
   - 10 consecutive days of features ‚Üí next day's close price
   - Results in 57 - 10 = 47 training samples

### Example 3: Training LSTM Model

```python
from data_processor import DataProcessor
from model import StockPricePredictor

processor = DataProcessor()

# Load and prepare data
df = processor.load_processed_data("TCS")
X, y = processor.create_sequences(df, sequence_length=10)
X = processor.normalize_data(X, fit=True)
X_train, X_val, X_test, y_train, y_val, y_test = processor.split_dataset(X, y)

# Build and train model
model = StockPricePredictor(input_shape=(10, 13))
results = model.train(X_train, y_train, X_val, y_val, symbol="TCS")

# Evaluate
metrics = model.evaluate(X_test, y_test)
print(f"Test RMSE: {metrics['rmse']:.4f}")
print(f"Test MAPE: {metrics['mape']:.2f}%")

# Make predictions
predictions = model.predict(X_test)
print(predictions.shape)  # (5,) - predictions for 5 test samples

# Save model
model.save("TCS")
```

**What happens:**
1. Build LSTM architecture (2 layers, 64 units)
2. Compile with Adam optimizer, MSE loss
3. Train for up to 50 epochs:
   - Epoch 1-5: Large loss decrease
   - Epoch 5-20: Loss stabilizes
   - Epoch 20+: Minimal improvement, early stopping may trigger
4. Save best model to `models/TCS.h5`
5. Save metadata (loss, metrics) to `models/TCS.json`

---

## ‚ö†Ô∏è Common Issues & Solutions

### Issue: "INDIANAPI_KEY not set"
```python
ValueError: INDIANAPI_KEY environment variable not set.
```

**Solution:**
```bash
export INDIANAPI_KEY="your_actual_key"
python main.py --status
```

Verify:
```bash
python -c "import os; print(os.getenv('INDIANAPI_KEY'))"
```

### Issue: "Daily request limit reached"
```
Daily request limit reached (10).
Requests reset at 0:00 UTC.
```

**Solution:**
- Wait until tomorrow (quota resets at midnight UTC)
- Or reduce `MAX_REQUESTS_PER_DAY` in config.py
- Or collect fewer symbols today

**Check remaining quota:**
```bash
python main.py --status
```

### Issue: "No raw data file found for TCS"
```
No raw data file found for TCS
Failed symbols: ['TCS']
```

**Solution:**
```bash
# Must collect data first
python main.py --collect

# Then process
python main.py --process
```

### Issue: "Invalid raw data structure for TCS"
```
Error processing raw data for TCS: Invalid raw data structure
```

**Solution:**
1. Check actual JSON structure in `data/raw/TCS_raw.json`
2. Update parsing in `data_processor.py` lines 50-70:
   ```python
   # Your JSON might be:
   # {"data": [...]}           or
   # {"candles": [...]}        or
   # {"OHLC": [...]}
   
   # Update column_mapping to match actual structure
   column_mapping = {
       "actual_field_name": "timestamp",
       "o": "open",
       ...
   }
   ```

### Issue: "TensorFlow not installed"
```
ImportError: TensorFlow is required.
```

**Solution:**
```bash
pip install tensorflow
# or
pip install -r requirements.txt
```

### Issue: "Insufficient data for symbol"
```
Insufficient data for TCS: 15 points < minimum 30
```

**Solution:**
- Increase `HISTORY_DAYS` in config.py
- Some symbols may have limited historical data available
- Try different symbols (larger companies have more data)

---

## üéì Learning Resources

### Understanding the Code Flow

**Typical User Journey:**
```
1. Set INDIANAPI_KEY env variable
2. python main.py --status
   ‚Üí Check remaining quota
3. python main.py --collect
   ‚Üí Fetch data from API (uses 1 request per symbol)
   ‚Üí Save raw JSON locally
4. python main.py --process
   ‚Üí Load JSON, compute indicators
   ‚Üí Save processed CSV (no API calls)
5. python main.py --train
   ‚Üí Load processed CSV
   ‚Üí Create sequences, train LSTM
   ‚Üí Save models (no API calls)
6. python main.py --train (again)
   ‚Üí Uses same CSV, trains new model
   ‚Üí No API calls, no quota used
7. Next day ‚Üí python main.py --full
   ‚Üí Collect fresh data, retrain models
```

### Time-Series Modeling Concepts

**Sequence-to-Value Approach:**
```
Input Sequence (10 days):
Day 1: [open: 100, high: 105, low: 99, ..., rsi: 65]
Day 2: [open: 102, high: 107, low: 101, ..., rsi: 67]
...
Day 10: [open: 115, high: 118, low: 114, ..., rsi: 72]
       ‚Üì
LSTM processes temporal patterns
       ‚Üì
Output: Predicted close price Day 11: ~117.50
```

**Why This Works:**
- LSTM captures temporal dependencies
- "If prices rising for 10 days with low RSI, likely to continue"
- Multiple features provide context (not just close price)

### Rate Limiting Design

**Why 10 requests/day for 500/month quota:**
```
500 requests / 30 days = 16.7 requests/day average
But we use 10/day (conservative) for:
- Safety margin if you run pipeline multiple days
- Manual API calls for testing
- Buffer for unexpected overages

Example month:
Day 1: 5 requests (collected 5 symbols)
Day 2: 0 requests (processed and trained)
Day 3: 5 requests (collected new data)
Day 4-29: 0 requests (training only)
Total: 10/500 = 2% usage, safe buffer
```

---

## üìà Extension Ideas

### Add More Symbols
```python
# config.py
SYMBOLS = [
    "TCS", "HDFC", "RELIANCE", "WIPRO", "INFY",  # Original 5
    "SUNPHARMA", "BAJAJFINSV", "JSWSTEEL",       # Add more
]
```

### Add Custom Technical Indicators
```python
# data_processor.py - add to compute_technical_indicators()
# MACD (Moving Average Convergence Divergence)
exp1 = df['close'].ewm(span=12, adjust=False).mean()
exp2 = df['close'].ewm(span=26, adjust=False).mean()
df['macd'] = exp1 - exp2

# Bollinger Bands
df['bb_middle'] = df['close'].rolling(20).mean()
df['bb_std'] = df['close'].rolling(20).std()
df['bb_upper'] = df['bb_middle'] + 2 * df['bb_std']
df['bb_lower'] = df['bb_middle'] - 2 * df['bb_std']
```

### Use Different Model Architectures
```python
# model.py - replace LSTM with:
layers.GRU(64, return_sequences=True)  # GRU (simpler, faster)
layers.Bidirectional(layers.LSTM(64))   # Bidirectional LSTM
# Or Transformer architecture for longer sequences
```

### Enable Prediction on New Data
```python
# After training, predict next day's price
model = StockPricePredictor.load("TCS")

# Get latest 10 days
latest_data = df.tail(10)
X_new = create_sequences(latest_data, sequence_length=1)
prediction = model.predict(X_new)[0]
print(f"Predicted close price: {prediction:.2f}")
```

---

## üìö Summary

| Component | File | Purpose | API Calls |
|-----------|------|---------|-----------|
| **Configuration** | `config.py` | Central params, API keys, symbols | 0 |
| **API Client** | `api_client.py` | Fetch data, rate limiting | 1+ |
| **Data Processor** | `data_processor.py` | Features, indicators, sequences | 0 |
| **Model** | `model.py` | LSTM training, evaluation | 0 |
| **Orchestration** | `main.py` | CLI, pipeline coordination | 0-1+ |

**Total API Calls Required:**
- `--collect`: 1 per symbol (TCS, HDFC, etc.)
- `--process`: 0 (uses saved data)
- `--train`: 0 (uses saved data)
- `--full`: 1 per symbol

**Monthly Budget:**
- Collect 5 symbols daily: 5 √ó 30 = 150 requests
- Leaves 350 requests for testing, development, new symbols

---

## ‚úÖ Checklist

Before running in production:

- [ ] INDIANAPI_KEY environment variable set
- [ ] Updated API endpoints in `config.py`
- [ ] Verified symbol format for your API
- [ ] Tested with `python main.py --status`
- [ ] Tested collecting 1 symbol with `python main.py --collect`
- [ ] Verified data saved to `data/raw/`
- [ ] Tested processing with `python main.py --process`
- [ ] Verified CSV saved to `data/processed/`
- [ ] TensorFlow installed: `pip install tensorflow`
- [ ] Tested training with `python main.py --train`
- [ ] Verified model saved to `models/`

---

**Created:** December 18, 2025  
**Status:** Production Ready  
**Version:** 1.0.0
